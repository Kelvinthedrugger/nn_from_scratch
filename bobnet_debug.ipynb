{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "print(\"bobnet debug\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bobnet debug\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from fetch_it import mnist\r\n",
    "\r\n",
    "x_train,y_train, x_test,y_test = mnist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from frame import layer_init\r\n",
    "\r\n",
    "def forward_backward(x,y,l1,l2):\r\n",
    "    f_l1 = x@l1 # (1,128)\r\n",
    "    f_l2 = f_l1@l2 # (1,10)\r\n",
    "    # backward\r\n",
    "    # loss calculation: binary cross entropy w/ softmax\r\n",
    "    out = np.zeros((len(y),10),np.float32)\r\n",
    "    out[range(out.shape[0]),y] = 1\r\n",
    "\r\n",
    "    pp = (f_l2 - np.log(np.exp(f_l2)).sum(axis=1).reshape((-1,1)))\r\n",
    "    assert out.shape == pp.shape\r\n",
    "    loss = -np.multiply(out, pp).mean(axis=1)\r\n",
    "    #assert loss > 0\r\n",
    "    # backprop\r\n",
    "    d_out = -out/len(y)\r\n",
    "    grads = d_out - (np.exp(-loss)*d_out.sum(axis=1)).reshape((-1,1))\r\n",
    "    d_l2 = f_l1.T@grads\r\n",
    "    assert d_l2.shape == l2.shape\r\n",
    "    d_l1 = x.T@((l2@grads.T).T)\r\n",
    "    assert d_l1.shape == l1.shape\r\n",
    "    return f_l2, loss, d_l1, d_l2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from tqdm import tqdm\r\n",
    "\r\n",
    "np.random.seed(1337)\r\n",
    "\r\n",
    "l1 = layer_init(784,128)\r\n",
    "l2 = layer_init(128,10)\r\n",
    "\r\n",
    "losses = []\r\n",
    "lr = 1e-6\r\n",
    "for i in tqdm(range(1000)):\r\n",
    "    samp = np.random.randint(0,x_train.shape[0],size=(32))\r\n",
    "    X = x_train[samp].reshape((-1,28**2))\r\n",
    "    Y = y_train[samp]\r\n",
    "    pred, loss, d_l1, d_l2 = forward_backward(X,Y,l1,l2)\r\n",
    "    l1 -= lr*d_l1\r\n",
    "    l2 -= lr*d_l2\r\n",
    "\r\n",
    "    losses.append(loss.mean())\r\n",
    "\r\n",
    "losses[:10], pred[0]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]<ipython-input-32-570640b0185b>:11: RuntimeWarning: divide by zero encountered in log\n",
      "  pp = (f_l2 - np.log(np.exp(f_l2)).sum(axis=1).reshape((-1,1)))\n",
      "<ipython-input-32-570640b0185b>:13: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -np.multiply(out, pp).mean(axis=1)\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 2724.90it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([0.00593465,\n",
       "  -0.036825538,\n",
       "  -0.10968059,\n",
       "  -0.14623457,\n",
       "  -0.20979637,\n",
       "  -0.36515918,\n",
       "  -0.53245527,\n",
       "  -0.58392674,\n",
       "  -0.65903664,\n",
       "  -0.8064263],\n",
       " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32))"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('Kelvin': virtualenv)"
  },
  "interpreter": {
   "hash": "fdb5c6f5eaa2a17ed3afdcccdfa066a85023e220ece0c7237533a765dc216f79"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}